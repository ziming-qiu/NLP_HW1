{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 70000\n",
    "MAX_SENTENCE_LENGTH = 800\n",
    "emb_dim = 100\n",
    "n_gram_model = 2\n",
    "optimizer_flag = 1 #0 for SGD, 1 for Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./aclImdb/train/neg\n",
      "./aclImdb/train/pos\n",
      "./aclImdb/test/neg\n",
      "./aclImdb/test/pos\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "def load_train_test_imdb_data(data_dir):\n",
    "    \"\"\"Loads the IMDB train/test datasets from a folder path.\n",
    "    Input:\n",
    "    data_dir: path to the \"aclImdb\" folder.\n",
    "    \n",
    "    Returns:\n",
    "    train/test datasets as pandas dataframes.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        data[split] = []\n",
    "        for sentiment in [\"neg\", \"pos\"]:\n",
    "            score = 1 if sentiment == \"pos\" else 0\n",
    "\n",
    "            path = os.path.join(data_dir, split, sentiment)\n",
    "            print(path)\n",
    "            file_names = os.listdir(path)\n",
    "            for f_name in file_names:\n",
    "                with open(os.path.join(path, f_name), \"r\") as f:\n",
    "                    review = f.read()\n",
    "                    data[split].append([review, score])\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(data[\"train\"])  \n",
    "    np.random.shuffle(data[\"test\"])\n",
    "    train_data = []\n",
    "    train_targets = []\n",
    "    test_data = []\n",
    "    test_targets = []\n",
    "    for i in range(len(data[\"train\"])):\n",
    "        train_data.append(data[\"train\"][i][0])\n",
    "        train_targets.append(data[\"train\"][i][1])\n",
    "    \n",
    "    for i in range(len(data[\"test\"])):\n",
    "        test_data.append(data[\"test\"][i][0])\n",
    "        test_targets.append(data[\"test\"][i][1])\n",
    "    return train_data, train_targets, test_data, test_targets\n",
    "\n",
    "all_train_data, all_train_targets, test_data, test_targets = load_train_test_imdb_data(data_dir=\"./aclImdb\")\n",
    "train_split = 20000\n",
    "train_data = all_train_data[:train_split]\n",
    "train_targets = all_train_targets[:train_split]\n",
    "\n",
    "val_data = all_train_data[train_split:]\n",
    "val_targets = all_train_targets[train_split:]\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'be', 'look', 'at', 'buy', 'u.k.', 'startup', 'for', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "#def tokenize(sent):\n",
    "#  tokens = tokenizer(sent)\n",
    "#  return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "\n",
    "def tokenize(sent):\n",
    "   tokens = tokenizer(sent)\n",
    "   return [token.lemma_.lower() for token in tokens if (token.lemma_ not in punctuations)]\n",
    "\n",
    "# Example\n",
    "tokens = tokenize(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code cell that tokenizes train/val/test datasets\n",
    "# However it takes about 15-20 minutes to run it\n",
    "# For convinience we have provided the preprocessed datasets\n",
    "# Please see the next code cell\n",
    "import pickle as pkl\n",
    "\n",
    "def tokenize_dataset(dataset, ngram=1):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    if ngram == 1:\n",
    "        for sample in tqdm(dataset):\n",
    "            tokens = tokenize(sample)\n",
    "            token_dataset.append(tokens)\n",
    "            all_tokens += tokens\n",
    "    elif ngram == 2:\n",
    "        for sample in tqdm(dataset):\n",
    "            tokens = tokenize(sample)\n",
    "            bigram_tokens = []\n",
    "            for i in range(len(tokens)-1):\n",
    "                bigram_tokens.append(tokens[i] + ' '+ tokens[i+1])\n",
    "            token_dataset.append(bigram_tokens)\n",
    "            all_tokens += bigram_tokens\n",
    "    elif ngram == 3:\n",
    "        for sample in tqdm(dataset):\n",
    "            tokens = tokenize(sample)\n",
    "            trigram_tokens = []\n",
    "            for i in range(len(tokens)-2):\n",
    "                trigram_tokens.append(tokens[i] + ' '+ tokens[i+1] + ' ' + tokens[i+2])\n",
    "            token_dataset.append(trigram_tokens)\n",
    "            all_tokens += trigram_tokens\n",
    "    elif ngram == 4:\n",
    "        for sample in tqdm(dataset):\n",
    "            tokens = tokenize(sample)\n",
    "            fourgram_tokens = []\n",
    "            for i in range(len(tokens)-3):\n",
    "                fourgram_tokens.append(tokens[i] + ' '+ tokens[i+1] + ' ' + tokens[i+2] + ' ' + tokens[i+3])\n",
    "            token_dataset.append(fourgram_tokens)\n",
    "            all_tokens += fourgram_tokens\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "def n_gram_transform(tokens_data, ngram=1):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    print(\"{}-gram model\".format(ngram))\n",
    "    if ngram == 1:\n",
    "        for i in tqdm(range(len(tokens_data))):\n",
    "            tokens = tokens_data[i]\n",
    "            token_dataset.append(tokens)\n",
    "            all_tokens += tokens\n",
    "    elif ngram == 2:\n",
    "        for i in tqdm(range(len(tokens_data))):\n",
    "            tokens = tokens_data[i]\n",
    "            bigram_tokens = []\n",
    "            for i in range(len(tokens)-1):\n",
    "                bigram_tokens.append(tokens[i] + ' '+ tokens[i+1])\n",
    "            token_dataset.append(bigram_tokens)\n",
    "            all_tokens += bigram_tokens\n",
    "    elif ngram == 3:\n",
    "        for i in tqdm(range(len(tokens_data))):\n",
    "            tokens = tokens_data[i]\n",
    "            trigram_tokens = []\n",
    "            for i in range(len(tokens)-2):\n",
    "                trigram_tokens.append(tokens[i] + ' '+ tokens[i+1] + ' ' + tokens[i+2])\n",
    "            token_dataset.append(trigram_tokens)\n",
    "            all_tokens += trigram_tokens\n",
    "    elif ngram == 4:\n",
    "        for i in tqdm(range(len(tokens_data))):\n",
    "            tokens = tokens_data[i]\n",
    "            fourgram_tokens = []\n",
    "            for i in range(len(tokens)-3):\n",
    "                fourgram_tokens.append(tokens[i] + ' '+ tokens[i+1] + ' ' + tokens[i+2] + ' ' + tokens[i+3])\n",
    "            token_dataset.append(fourgram_tokens)\n",
    "            all_tokens += fourgram_tokens\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_data, 1)\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens_new_lemma.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_data, 1)\n",
    "pkl.dump(test_data_tokens, open(\"test_data_tokens_new_lemma.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(train_data, 1)\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens_new_lemma.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens_new_lemma.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(all_train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4811775\n"
     ]
    }
   ],
   "source": [
    "# First, download datasets from here\n",
    "# Use your NYU account\n",
    "#https://drive.google.com/open?id=1eR2LFI5MGliHlaL1S2nsX4ouIO1k_ip2\n",
    "#https://drive.google.com/open?id=133QCWbiz_Xc7Qm4r6t-fJP1K669xjNlM\n",
    "#https://drive.google.com/open?id=1SuUIUpJ1iznU707ktkpnEGSwt_XIqOYp\n",
    "#https://drive.google.com/open?id=1UQsrZ2LVfcxdxxa47344fMs_qvya72KR\n",
    "\n",
    "\n",
    "# Then, load preprocessed train, val and test datasets\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens_new_lemma.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens_new_lemma.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens_new_lemma.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens_new_lemma.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1291/5000 [00:00<00:00, 12909.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "2-gram model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 13001.22it/s]\n",
      "  6%|▌         | 1456/25000 [00:00<00:01, 14548.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n",
      "2-gram model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:02<00:00, 12056.94it/s]\n",
      "  7%|▋         | 1460/20000 [00:00<00:01, 14434.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n",
      "2-gram model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:01<00:00, 12727.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = n_gram_transform(val_data_tokens, n_gram_model)\n",
    "#pkl.dump(val_data_tokens, open(\"val_data_tokens_new_2.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = n_gram_transform(test_data_tokens, n_gram_model)\n",
    "#pkl.dump(test_data_tokens, open(\"test_data_tokens_new_2.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = n_gram_transform(train_data_tokens, n_gram_model)\n",
    "#pkl.dump(train_data_tokens, open(\"train_data_tokens_new_2.p\", \"wb\"))\n",
    "#pkl.dump(all_train_tokens, open(\"all_train_tokens_new_2.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036997\n",
      "4791775\n"
     ]
    }
   ],
   "source": [
    "print(len(set(all_train_tokens)))\n",
    "print(len(all_train_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 12032 ; token the cake\n",
      "Token the cake; token id 12032\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "random_token_id = np.random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create PyTorch DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   39,    87,   127,  ...,     0,     0,     0],\n",
      "        [48179,   498, 23209,  ...,     0,     0,     0],\n",
      "        [ 2166, 15498,   470,  ...,    87,    83,  1245],\n",
      "        ...,\n",
      "        [ 6574, 30903, 21645,  ...,     0,     0,     0],\n",
      "        [    1,     1,     1,  ...,     0,     0,     0],\n",
      "        [ 2139, 68080,     1,  ...,     0,     0,     0]])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        1, 0, 1, 1, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "    print (data)\n",
    "    print (labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define Bag-of-Words model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "    \n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose Adam as optimizer\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 79.46, validation_num: 5000\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 85.56, validation_num: 5000\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 86.16, validation_num: 5000\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 87.22, validation_num: 5000\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 87.92, validation_num: 5000\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 88.68, validation_num: 5000\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 88.9, validation_num: 5000\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 88.9, validation_num: 5000\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 89.18, validation_num: 5000\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 88.42, validation_num: 5000\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 88.46, validation_num: 5000\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 88.46, validation_num: 5000\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 88.38, validation_num: 5000\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 88.62, validation_num: 5000\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 88.46, validation_num: 5000\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 88.72, validation_num: 5000\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 88.68, validation_num: 5000\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 87.82, validation_num: 5000\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 88.46, validation_num: 5000\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 88.54, validation_num: 5000\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 88.6, validation_num: 5000\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 88.7, validation_num: 5000\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 88.68, validation_num: 5000\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 88.62, validation_num: 5000\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 88.58, validation_num: 5000\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 88.7, validation_num: 5000\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 88.66, validation_num: 5000\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 88.6, validation_num: 5000\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 88.58, validation_num: 5000\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 88.68, validation_num: 5000\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 88.66, validation_num: 5000\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 88.6, validation_num: 5000\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 88.6, validation_num: 5000\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 88.7, validation_num: 5000\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 88.66, validation_num: 5000\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 88.58, validation_num: 5000\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 88.72, validation_num: 5000\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 88.74, validation_num: 5000\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 88.72, validation_num: 5000\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 88.68, validation_num: 5000\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 88.64, validation_num: 5000\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 88.62, validation_num: 5000\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 88.62, validation_num: 5000\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 88.68, validation_num: 5000\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 88.7, validation_num: 5000\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 88.64, validation_num: 5000\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 88.68, validation_num: 5000\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 88.66, validation_num: 5000\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 88.58, validation_num: 5000\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 88.6, validation_num: 5000\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 88.58, validation_num: 5000\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 88.64, validation_num: 5000\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 88.68, validation_num: 5000\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 88.72, validation_num: 5000\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 88.7, validation_num: 5000\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 88.68, validation_num: 5000\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 88.66, validation_num: 5000\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 88.68, validation_num: 5000\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 88.66, validation_num: 5000\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 88.66, validation_num: 5000\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if optimizer_flag == 0:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    print('choose SGD as optimizer')\n",
    "elif optimizer_flag == 1:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print('choose Adam as optimizer')\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "loss_list =[]\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "        \n",
    "    return (100 * correct / total, total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss_list.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_num = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, validation_num: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc, val_num))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "imshow() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-64b18d76a5b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: imshow() missing 1 required positional argument: 'X'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1265d8278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_list)\n",
    "plt.ylabel('training loss')\n",
    "plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 88.68\n",
      "Test Acc 87.556\n",
      "Train Acc 100.0\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)[0]))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)[0]))\n",
    "print (\"Train Acc {}\".format(test_model(train_loader, model)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_num = 3\n",
    "good_sample = []\n",
    "good_label = []\n",
    "bad_num = 3\n",
    "bad_sample = []\n",
    "bad_label = []\n",
    "pre_label = []\n",
    "for data, lengths, labels in val_loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        for i in range(len(predicted)):\n",
    "            if predicted[i,0] == label_batch[i]:\n",
    "                good_sample.append(data_batch[i])\n",
    "                good_label.append(label_batch[i])\n",
    "                good_num+=1\n",
    "            else:\n",
    "                bad_sample.append(data_batch[i])\n",
    "                bad_label.append(label_batch[i])\n",
    "                pre_label.append(predicted[i,0])\n",
    "                bad_num+=1\n",
    "        if good_num > 10 and bad_num > 10:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(good_sample)):\n",
    "    sample = []\n",
    "    for j in range(len(good_sample[i])):\n",
    "        sample.append(id2token[good_sample[i][j]])\n",
    "    print(good_label[i])\n",
    "    print(sample)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bad_sample)):\n",
    "    sample = []\n",
    "    for j in range(len(bad_sample[i])):\n",
    "        current_id = int(bad_sample[i][j])\n",
    "        sample.append(id2token[current_id])\n",
    "    print(bad_label[i])\n",
    "    print(pre_label[i])\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
